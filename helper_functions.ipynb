{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a6e0f3-c354-4a62-bcaf-9a41f1dc09d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, Image\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return Image.from_bytes(image_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1a7c7-6d9c-463a-96d2-8014c94b93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "\n",
    "import IPython.display\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: typing.Iterable[Image],\n",
    "    max_width: int = 600,\n",
    "    max_height: int = 350,\n",
    ") -> None:\n",
    "    for image in images:\n",
    "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "        if pil_image.mode != \"RGB\":\n",
    "            # RGB is supported by all Jupyter environments (e.g. RGBA is not yet)\n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "        image_width, image_height = pil_image.size\n",
    "        if max_width < image_width or max_height < image_height:\n",
    "            # Resize to display a smaller notebook image\n",
    "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "        IPython.display.display(pil_image)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def display_content_as_image(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Image):\n",
    "        return False\n",
    "    display_images([content])\n",
    "    return True\n",
    "\n",
    "\n",
    "def display_content_as_video(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Part):\n",
    "        return False\n",
    "    part = typing.cast(Part, content)\n",
    "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
    "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
    "    return True\n",
    "\n",
    "\n",
    "def print_multimodal_prompt(contents: list[str | Image | Part]):\n",
    "    \"\"\"\n",
    "    Given contents that would be sent to Gemini,\n",
    "    output the full multimodal prompt for ease of readability.\n",
    "    \"\"\"\n",
    "    for content in contents:\n",
    "        if display_content_as_image(content):\n",
    "            continue\n",
    "        if display_content_as_video(content):\n",
    "            continue\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4cac6-5182-4820-b02d-e67b7ce79ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image metadata\n",
    "import json\n",
    "import os\n",
    "\n",
    "metadatas = {}\n",
    "for file_name in os.listdir(image_directory):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(image_directory, file_name)) as f:\n",
    "            metadata = json.load(f)\n",
    "            metadatas.update(metadata)\n",
    "\n",
    "image_names = list(metadatas.keys())\n",
    "image_paths = [os.path.join(image_directory, image_name) for image_name in image_names]\n",
    "\n",
    "len(metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48feab7-fa61-489e-8557-c70444438896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#embedding with API instead of SDK\n",
    "import base64\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import struct_pb2\n",
    "import sys\n",
    "import time\n",
    "import typing\n",
    "\n",
    "PROJECT_ID = '' # @param {type: \"string\"}\n",
    "\n",
    "# Inspired from https://stackoverflow.com/questions/34269772/type-hints-in-namedtuple.\n",
    "class EmbeddingResponse(typing.NamedTuple):\n",
    "    text_embedding: typing.Sequence[float]\n",
    "    image_embedding: typing.Sequence[float]\n",
    "\n",
    "class EmbeddingPredictionClient:\n",
    "  # \"\"\"Wrapper around Prediction Service Client.\"\"\"\n",
    "    def __init__(self, project : str,\n",
    "        location : str = \"us-central1\",\n",
    "        api_regional_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\n",
    "        client_options = {\"api_endpoint\": api_regional_endpoint}\n",
    "        # Initialize client that will be used to create and send requests.\n",
    "        # This client only needs to be created once, and can be reused for multiple requests.\n",
    "        self.client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "        self.location = location\n",
    "        self.project = project\n",
    "    \n",
    "    def get_embedding(self, text : str = None, image_bytes : bytes = None):\n",
    "        if not text and not image_bytes:\n",
    "            raise ValueError('At least one of text or image_bytes must be specified.')\n",
    "\n",
    "        instance = struct_pb2.Struct()\n",
    "        if text:\n",
    "            instance.fields['text'].string_value = text\n",
    "\n",
    "        if image_bytes:\n",
    "            encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "            image_struct = instance.fields['image'].struct_value\n",
    "            image_struct.fields['bytesBase64Encoded'].string_value = encoded_content\n",
    "\n",
    "        instances = [instance]\n",
    "        endpoint = (f\"projects/{self.project}/locations/{self.location}\"\"/publishers/google/models/multimodalembedding@001\")\n",
    "        response = self.client.predict(endpoint=endpoint, instances=instances)\n",
    "\n",
    "        text_embedding = None\n",
    "        if text:\n",
    "            text_emb_value = response.predictions[0]['textEmbedding']\n",
    "            text_embedding = [v for v in text_emb_value]\n",
    "\n",
    "        image_embedding = None\n",
    "        if image_bytes:\n",
    "            image_emb_value = response.predictions[0]['imageEmbedding']\n",
    "            image_embedding = [v for v in image_emb_value]\n",
    "\n",
    "        return EmbeddingResponse(text_embedding=text_embedding,image_embedding=image_embedding)\n",
    "\n",
    "client = EmbeddingPredictionClient(project=PROJECT_ID)\n",
    "\n",
    "# Extract image embedding\n",
    "def getImageEmbeddingFromImageContent(content):\n",
    "    response = client.get_embedding(text=None, image_bytes=content)\n",
    "    return response.image_embedding\n",
    "\n",
    "def getImageEmbeddingFromGcsObject(gcsBucket, gcsObject):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(gcsBucket)\n",
    "    blob = bucket.blob(gcsObject)\n",
    "\n",
    "    with blob.open(\"rb\") as f:\n",
    "        return getImageEmbeddingFromImageContent(f.read())\n",
    "\n",
    "def getImageEmbeddingFromFile(filePath):\n",
    "    with open(filePath, \"rb\") as f:\n",
    "        return getImageEmbeddingFromImageContent(f.read())\n",
    "\n",
    "# Extract text embedding\n",
    "def getTextEmbedding(text):\n",
    "    response = client.get_embedding(text=text, image_bytes=None)\n",
    "    return response.text_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70d15e-7c64-4a33-a1c5-2f92c2ee3601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
